#!/usr/bin/env python3

import requests
import json
import os
import time
import re
from urllib.parse import urlparse, urlunparse
from bs4 import BeautifulSoup

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BASE_URL = os.getenv("FRESHRSS_URL", "").rstrip('/')
USER = os.getenv("FRESHRSS_USER")
PASS = os.getenv("FRESHRSS_PASS")
OPENROUTER_KEY = os.getenv("OPENROUTER_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
CATEGORIES_AI = [c.strip() for c in os.getenv("CATEGORIES_AI", "").split(",") if c.strip()]
CATEGORIES_DIRECT = [c.strip() for c in os.getenv("CATEGORIES_DIRECT", "").split(",") if c.strip()]

DB_FILE = "seen_urls.txt"

AI_MODELS = [
    "google/gemini-2.0-flash-exp:free",
    "meta-llama/llama-3.3-70b-instruct:free",
    "mistralai/mistral-small-3.1-24b-instruct:free"
]

def log(message):
    print(f"[{time.strftime('%H:%M:%S')}] {message}")

def normalize_url(url):
    try:
        parsed = urlparse(url)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
    except: return url

def make_hashtag(text):
    # –û—á–∏—Å—Ç–∫–∞: —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã (–≤–∫–ª—é—á–∞—è —ë) –∏ —Ü–∏—Ñ—Ä—ã
    clean = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]', '', text)
    return f"#{clean}" if clean else ""

def get_clean_channel_tag(text):
    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ (—É–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω–∏–µ —Ö–≤–æ—Å—Ç—ã)
    text = re.sub(r'(?i)\s*(youtube|telegramchannel)\s*$', '', text).strip()
    return make_hashtag(text)

def get_domain_tag(url):
    try:
        domain = urlparse(url).netloc.lower()
        tag = domain.replace('www.', '').split('.')[0].replace('-', '')
        return f"#{tag}"
    except: return "#news"

def load_seen():
    if os.path.exists(DB_FILE):
        with open(DB_FILE, "r") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_seen(seen_set):
    list_to_save = list(seen_set)[-1000:]
    with open(DB_FILE, "w") as f:
        for item in list_to_save: f.write(f"{item}\n")

def get_auth_token():
    url = f"{BASE_URL}/api/greader.php/accounts/ClientLogin"
    try:
        r = requests.get(url, params={'Email': USER, 'Passwd': PASS}, timeout=10)
        if r.status_code == 200:
            for line in r.text.split('\n'):
                if line.startswith('Auth='): return line.replace('Auth=', '').strip()
    except Exception as e: log(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ FreshRSS: {e}")
    return None

def get_full_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return ""
        soup = BeautifulSoup(r.text, 'html.parser')
        for s in soup(['script', 'style', 'nav', 'header', 'footer']): s.decompose()
        article = (soup.find('div', {'class': 'tm-article-body'}) or
                   soup.find('div', {'class': 'tgme_widget_message_text'}) or
                   soup.find('article') or soup.find('main'))
        text = article.get_text(separator=' ', strip=True) if article else soup.get_text(separator=' ', strip=True)
        clean_text = " ".join(text.split())[:4500]
        log(f"üîé –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω ({len(clean_text)} —Å–∏–º–≤.): {url}")
        return clean_text
    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
        return ""

def get_ai_summary(url):
    content = get_full_text(url)
    if len(content) < 150:
        log(f"‚è© –ü—Ä–æ–ø—É—Å–∫: —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(content)} —Å–∏–º–≤.)")
        return None

    prompt = "–°—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –û–î–ù–ò–ú –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º (–¥–æ 25 —Å–ª–æ–≤) –Ω–∞ –†–£–°–°–ö–û–ú. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã."

    for model in AI_MODELS:
        try:
            log(f"ü§ñ –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏: {model}")
            r = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {OPENROUTER_KEY}", "Content-Type": "application/json"},
                data=json.dumps({"model": model, "messages": [{"role": "user", "content": f"{prompt}\n\n{content}"}], "temperature": 0.1}),
                timeout=35
            )
            if r.status_code == 200:
                summary = r.json()['choices'][0]['message']['content'].strip().rstrip('.')
                log(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç {model}")
                return summary
            else:
                log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model}: {r.status_code} {r.text[:100]}")
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {model}: {e}")
            continue
    return None

def send_tg(text, preview=None):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML"
    }
    if preview:
        payload["link_preview_options"] = json.dumps(preview)
    else:
        payload["link_preview_options"] = json.dumps({"is_disabled": True})

    try:
        r = requests.post(url, data=payload, timeout=10)
        if r.status_code == 200:
            log("üì≤ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram")
            return True
        else:
            log(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {r.status_code} {r.text}")
    except Exception as e:
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ TG: {e}")
    return False

def process_category(cat_name, use_ai, token, headers, api_base, global_seen_urls):
    log(f"üöÄ –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ò: {cat_name.upper()}")
    try:
        r = requests.get(f"{api_base}/stream/contents/user/-/label/{cat_name}",
                         params={'xt': 'user/-/state/com.google/read', 'n': 40}, headers=headers)
        items = r.json().get('items', [])
        if not items:
            log(f"üì≠ –ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat_name} –ø—É—Å—Ç–∞")
            return

        cat_tag = make_hashtag(cat_name)
        ai_msg_body = f"{cat_tag}\n\n"
        ai_count = 0

        for item in items:
            raw_url = item.get('alternate', [{}])[0].get('href', '')
            link = normalize_url(raw_url)
            title = item.get('title', '–ù–æ–≤–æ—Å—Ç—å')
            source_name = item.get('origin', {}).get('title', 'news')

            if link in global_seen_urls:
                requests.post(f"{api_base}/edit-tag", headers=headers, data={'i': item.get('id'), 'a': 'user/-/state/com.google/read'})
                continue

            log(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {title[:50]}...")

            if use_ai:
                summary = get_ai_summary(link)
                if summary:
                    tag = get_domain_tag(link)
                    ai_msg_body += f"üìå <i>{summary}</i>\nüè∑Ô∏è <a href='{link}'>{tag}</a>\n\n"
                    ai_count += 1
            else:
                tag = get_clean_channel_tag(source_name)
                preview = {"url": link, "prefer_large_media": True, "show_above_text": True}
                direct_msg = f"üìå <a href='{link}'>{title}</a>\nüè∑Ô∏è <a href='{link}'>{tag}</a>"
                send_tg(direct_msg, preview)

            global_seen_urls.add(link)
            requests.post(f"{api_base}/edit-tag", headers=headers, data={'i': item.get('id'), 'a': 'user/-/state/com.google/read'})

        if use_ai and ai_count > 0:
            send_tg(ai_msg_body)

    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –≤ process_category: {e}")

def main():
    log("üèÅ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞")
    token = get_auth_token()
    if not token: return

    headers = {'Authorization': f'GoogleLogin auth={token}'}
    api_base = f"{BASE_URL}/api/greader.php/reader/api/0"
    global_seen_urls = load_seen()

    for cat in CATEGORIES_AI:
        process_category(cat, True, token, headers, api_base, global_seen_urls)
    for cat in CATEGORIES_DIRECT:
        process_category(cat, False, token, headers, api_base, global_seen_urls)

    save_seen(global_seen_urls)
    log("üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

if __name__ == "__main__":
    main()
